# -*- coding: utf-8 -*-
"""final_report.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kMSEmoffUobLpq8vWb9zsvas0xT78jqM

# **Relationship Between Airbnb Hosuing Price and Crime in NYC - 44**

Group Members:

Emre Eren 25139

Mehmet Kerem Özcoşar 25225

## Introduction



First of all, this project is aiming ***to find the relationship between price/
housing distribution of Airbnb rentals and crimes in NYC.*** To be able to achieve the results of this project, it was planned to divide it to sub-problems. Therefore,  **firstly**, ***we clean our first data set***, which is New York City Airbnb,  *in order to use only data that we needed*. Then we *manipulate this dataset in order to analyze New York City Airbnb* open data. Then, briefly,* we visualize this dataset to be able to interpret what are we handle *(for example, price distribution, house type distribution, distributions neighborhood by neighborhood etc.). After that part, we introduce additional dataset which is  NYPD Arrest Data (2019).  Like we did in previous part *we firstly clean our data according to what are our needs*. Then, again we visualize this data set to obtain crime distribution, crime rates neighbor by neighbor, which crimes are more common etc.

### Problem Definition

We try to find correlation between Airbnb price distribution with the rate of crime in the borough of NYC

### Utilezed Datasets
***First dataset*** that we used is New York City Airbnb open data. This dataset includes information about; listing id, name of listing, host id, host name, location, neighbourhood , latitude, longitude, room type, price (in dollars), amount of nights minimum, number of review, number of reviews per month, amount of listing per host and number of days when listing is available for booking.

***Second dataset*** is NYPD Arrest Data (2019). These datasets are explained more clearly in the parts that they introduced. This dataset includes a lot of information that we do not interested. We interested in only information about crime type, latitude, longitude and borough.
"""

from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd  # an alias for pandas
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from os.path import join 
# %matplotlib inline

"""#Data Exploration

## Airbnb Dataset
"""

data_path = "/content/gdrive/My Drive"
filename = "AB_NYC_2019.csv"

df = pd.read_csv(join(data_path, filename), delimiter=",")

df.head()

"""**CLEAN THE DATASET**

We move the unnecessary column in the dataframe and drop them in dataframe




"""

df.drop(["id","name","host_id","host_name","neighbourhood","last_review"], axis=1, inplace=True)
df.head()

"""Types of columns

"""

df.dtypes

"""Fınd the NA's in the dataframe

"""

df.isnull().sum()
#df.dropna(how="any")

"""Fill NAs in reviews per month with 0s

"""

df.fillna({"reviews_per_month":0}, inplace = True)
df.isnull().sum()

"""### Price Distribution

*   Let's try to find relationship between the price and the other information in dataframe

* As you can see in the below Manhattan has the more expensive house's prices rather than the other boroguhs of NYC.


"""

import seaborn as sns
fig, ax = plt.subplots(1,1, figsize=(10,10))
ax.set_xlim(0,750)
sub_r = df[df['neighbourhood_group'] == "Brooklyn"]["price"]
sub_m = df[df['neighbourhood_group'] == "Manhattan"]["price"]
sub_q = df[df['neighbourhood_group'] == "Queens"]["price"]
sub_b = df[df['neighbourhood_group'] == "Bronx"]["price"]
sub_s = df[df['neighbourhood_group'] == "Staten Island"]["price"]
sns.kdeplot(sub_r, shade=True, ax=ax, label="Brooklyn")
sns.kdeplot(sub_m, shade=True, ax=ax, label="Manhattan")
sns.kdeplot(sub_q, shade=True, ax=ax, label="Queens")
sns.kdeplot(sub_b, shade=True, ax=ax, label="Bronx")
sns.kdeplot(sub_s, shade=True, ax=ax, label="Staten Island")
plt.show()

"""DENSITY OF PRICE DISTRIBUTION IN BOROUGH

As you can see the shown below density of price distribution in Manhattan more narrow than others because price in Manhattan more changable and expensive than the other borough





"""

den_price=df[df.price < 500]
#using violinplot to showcase density and distribtuion of prices 
viz_2=sns.violinplot(data=den_price, x='neighbourhood_group', y='price')
viz_2.set_title('Density and distribution of prices for each neighberhood_group')

"""* As can you see most higher and the most wider plot belongs to Manhattan based on the minimum nights"""

fig, axes = plt.subplots(1,1, figsize=(10,10))
ax.set_xlim(0,750)

minn_r = df[df['neighbourhood_group'] == "Brooklyn"]["minimum_nights"]
minn_m = df[df['neighbourhood_group'] == "Manhattan"]["minimum_nights"]                 
minn_q = df[df['neighbourhood_group'] == "Queens"]["minimum_nights"]
minn_b = df[df['neighbourhood_group'] == "Bronx"]["minimum_nights"]
minn_s = df[df['neighbourhood_group'] == "Staten Island"]["minimum_nights"]

sns.kdeplot(minn_r, shade=True, ax=axes, label="Brooklyn")
sns.kdeplot(minn_m, shade=True, ax=axes, label="Manhattan")
sns.kdeplot(minn_q, shade=True, ax=axes, label="Queens")
sns.kdeplot(minn_b, shade=True, ax=axes, label="Bronx")
sns.kdeplot(minn_s, shade=True, ax=axes, label="Staten Island")
plt.show()

from collections import Counter
df_copy = df["room_type"]
counts = Counter(df_copy)
counts

plt.bar(counts.keys(), counts.values())
plt.show()

"""DISTRINBUTION OF ROOM TYPE IN THE NEIGHBOURHOOD GROUP

"""

import seaborn as sns
plt.figure(figsize=(15,6))
sns.countplot(data=df, x='neighbourhood_group', hue='room_type')
plt.title('Total Number of Airbnb in Neighbourhoods Group with Room Type Category', fontsize=17)
plt.xlabel('Neighbourhood Group')
plt.ylabel("Number of Room Types")
plt.legend(frameon=False, fontsize=12)

"""Number of Entire home in Manhattan is more than others and this can be the reason of why Manhattan has more expensive house, because price of entire room is more expensive than other room types"""

sub_7=df.loc[df['neighbourhood_group'].isin(['Brooklyn','Manhattan','Queens','Staten Island','Bronx'])]
#using catplot to represent multiple interesting attributes together and a count
viz_3=sns.catplot(x='neighbourhood_group', hue='neighbourhood_group', col='room_type', data=sub_7, kind='count')
viz_3.set_xticklabels(rotation=90)

data = df.neighbourhood_group.value_counts()[:10]
plt.figure(figsize=(12, 8))
x = list(data.index)
y = list(data.values)
x.reverse()
y.reverse()

plt.title("Most Popular Borough")
plt.ylabel("Neighbourhood Area")
plt.xlabel("Number of guest Who host in this Area")

plt.barh(x, y)

bins = np.linspace(1,365)

plt.hist(minn_r, bins,alpha = 0.9, label = "brooklyn")
plt.hist(minn_m,bins,alpha = 0.9, label = "manhattan")
plt.hist(minn_q,bins,alpha = 0.9, label = "queens")
plt.hist(minn_b, bins,alpha = 0.9, label = "bronx")
plt.hist(minn_s, bins,alpha = 0.9, label = "staten ısland")
plt.legend(loc = "upper right")
plt.show()

"""*Availibity of homes maybe the other affects of price distribution
because availibitly of home may increase the demand of home and it cause the increase the price of home*

"""

fig, ax = plt.subplots(1,1, figsize=(10,10))
ax.set_xlim(0,750)
ava_r = df[df['neighbourhood_group'] == "Brooklyn"]["availability_365"]
ava_m = df[df['neighbourhood_group'] == "Manhattan"]["availability_365"]
ava_q = df[df['neighbourhood_group'] == "Queens"]["availability_365"]
ava_b = df[df['neighbourhood_group'] == "Bronx"]["availability_365"]
ava_s = df[df['neighbourhood_group'] == "Staten Island"]["availability_365"]
sns.kdeplot(ava_r, shade=True, ax=ax, label="Brooklyn")
sns.kdeplot(ava_m, shade=True, ax=ax, label="Manhattan")
sns.kdeplot(ava_q, shade=True, ax=ax, label="Queens")
sns.kdeplot(ava_b, shade=True, ax=ax, label="Bronx")
sns.kdeplot(ava_s, shade=True, ax=ax, label="Staten Island")
plt.show()

"""PRICE DISTRIBUTION VISUALIZING WITH MAP

"""

plt.figure(figsize=(10,10))
sns.scatterplot(x='longitude', y='latitude', hue='neighbourhood_group',s=20, data=df)

viz_4=den_price.plot(kind='scatter', x='longitude', y='latitude', c='price', cmap=plt.get_cmap('jet'), colorbar=True, alpha=0.7, figsize=(10,10))
viz_4.legend()

"""* As the shown below Manhattan has the most expensive airbnb houses"""

import urllib
#initializing the figure size
plt.figure(figsize=(10,8))
#loading the png NYC image found on Google and saving to my local folder along with the project
i=urllib.request.urlopen('https://upload.wikimedia.org/wikipedia/commons/e/ec/Neighbourhoods_New_York_City_Map.PNG')
nyc_img=plt.imread(i)
#scaling the image based on the latitude and longitude max and mins for proper output
plt.imshow(nyc_img,zorder=0,extent=[-74.258, -73.7, 40.49,40.92])
ax=plt.gca()
#using scatterplot again
den_price.plot(kind='scatter', x='longitude', y='latitude', c='price', ax=ax, 
           cmap=plt.get_cmap('jet'), colorbar=True, alpha=0.7, zorder=5)
plt.legend()
plt.show()

"""**Correlation Map**

let's find the most significant factor of price distribution with correlation map


"""

plt.figure(figsize=(15,8))
sns.heatmap(df.corr(), annot=True, linewidths=0.1, cmap='Reds')

"""**Price has strong relationship with avaliability and calculated host listing count**"""

df.drop('price', axis=1).corrwith(df.price).plot.barh(figsize=(10, 7), 
                                                        title='Correlation with Response Variable',
                                                        fontsize=15, grid=True)

"""## Crime Dataset
 
"""

data_path2 = "/content/gdrive/My Drive"
filename = "NYPD_Arrest_Data__Year_to_Date_.csv"

crime = pd.read_csv(join(data_path2, filename), delimiter=",")
crimeX = crime # this will be used for to be able to use raw data

crime.head()

"""### Clean the Crime Datasets


*   Remove the null value in the columns
*   Remove the unnecessary columns in the data
*   Give more clear name for information in data


"""

crime.isnull().sum()

crime = crime[['ARREST_BORO','OFNS_DESC', 'Latitude', 'Longitude','ARREST_PRECINCT', 'X_COORD_CD', 'Y_COORD_CD' ]]
crime = crime.rename(columns={'OFNS_DESC':'Crime_desc'})
crime.fillna({"Crime_desc":0}, inplace = True)
crime.isnull().sum()

crime.head()

def changeName(ARREST_BORO):
  if ARREST_BORO == "M":
    return "Manhattan"
  if ARREST_BORO == "K":
    return "Brooklyn"
  if ARREST_BORO == "Q":
    return "Queens"
  if ARREST_BORO == "S":
    return "Staten Island"
  if ARREST_BORO == "B":
    return "Bronx"
crime["Borough"] = crime["ARREST_BORO"].apply(changeName)
crime.head()

crime.drop(["ARREST_BORO"], axis=1, inplace=True)
crime.head()

"""### Visualizing the Crime Dataset

Total Number of Crimes in Borough
"""

import seaborn as sns
plt.figure(figsize=(15,6))
sns.countplot(data=crime, x='Borough')
plt.title('Total Number of Crimes in Neighbourhoods Group', fontsize=17)
plt.xlabel('Neighbourhood Group')
plt.ylabel("Number of Crimes")
plt.legend(frameon=False, fontsize=12)

crime["Borough"].groupby(crime["Borough"]).count().sort_values(ascending=False).plot(kind="barh")

"""TOP 30 CRIMES IN NYC

"""

crime.groupby('Crime_desc').size().sort_values(ascending=False).head(30).plot(kind="bar")

"""TYPE OF CRIME DISTRIBUTION WITH RESPECT TO BOROUGH

* According to distinct features : 
 

*   Manhattan has traffic laws and petit larcency as distinct features
*   Brooklyn has criminal crimes as distinct features
*   Bronx has assult related offenses

* As can be seen in the plot Manhattan and Brooklyn have wider range of crime types
"""

sub_crime=crime.head(30).loc[crime['Borough'].isin(['Brooklyn','Manhattan','Queens','Staten Island','Bronx'])]
#using catplot to represent multiple interesting attributes together and a count
viz_crime=sns.catplot(x='Borough', hue='Crime_desc', data=sub_crime, kind='count')
viz_crime.set_xticklabels(rotation=90)

"""

* Number of crime that occurs borough by borough




"""

from collections import Counter
crime_copy = crime["Borough"]
count = Counter(crime_copy)
count

"""







*  Crime rates according to distinct boroughs



"""

sub = pd.Series(count)
population = {'Manhattan': 1629000, 'Brooklyn': 2533000, 'Queens': 2273000, 'Bronx': 1418000, 'Staten Island': 476143}
rate_m = count['Manhattan']/population['Manhattan']
rate_r = count['Brooklyn']/population['Brooklyn']
rate_q = count['Queens']/population['Queens']
rate_b = count['Bronx']/population['Bronx']
rate_s = count['Staten Island']/population['Staten Island']
print("Crime rate Manhattan:",rate_m, "Crime rate Brooklyn:",rate_r, "Crime rate Queens:",rate_q, "Crime rate Bronx:",rate_b, "Crime rate Staten Island:", rate_s)

"""

*  Piechart according to crime rates


---


1. With this piechart we are able to see which borugh has the highest crime rate and which has lowest crime rate.




"""

labels = 'Manhattan', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island'
sizes = [0.03309699201964395, 0.023011448874851955, 0.01960712714474263, 0.03442524682651622,0.018969091218394473 ]
explode = (0, 0, 0, 0, 0)

plt.pie(sizes, explode=explode, labels=labels, autopct='%.1f', shadow=True, startangle=90)
plt.axis('equal') 

plt.show()

"""

* Crime map by NYC's borughs

"""

plt.figure(figsize=(10,10))
sns.scatterplot(x='Longitude', y='Latitude', hue='Borough',s=20, data=crime)

"""

* Heat map of crime


---


1. We choose specifc type of crimes that can be effect housing/pricing distribution of airbnb
2. We can see which region is more dangerous 




"""

crimes_new = crime[(crime['Crime_desc'] == 'SEX CRIMES') | (crime['Crime_desc'] == 'HOMICIDE') | (crime['Crime_desc'] == 'ASSAULT 3') | 
                   (crime['Crime_desc'] == 'CRIMINAL')| (crime['Crime_desc'] == 'DANGEROUS DRUGS')]
ax = sns.lmplot('Longitude', 'Latitude',
                data= crimes_new[['Longitude','Latitude']],
                fit_reg=False,
                size=4, 
                scatter_kws={'alpha':.1})
ax = sns.kdeplot(crimes_new[['Longitude','Latitude']],
                 cmap="jet", 
                 bw=.005,
                 #n_levels=10,
                 cbar=True, 
                 shade=False, 
                 shade_lowest=False)
ax.set_xlim(-74.258, -73.7)
ax.set_ylim(40.49,40.92)

"""* As you can see in the heat map above Bronx has the more dangerous crime rates.

*  In this part again we see serious crimes on a map but different way
"""

df_deadly = crime[(crime['Crime_desc'] == 'ASSAULT 3 & RELATED OFFENSES') | (crime['Crime_desc'] == 'SEX CRIMES') | (crime['Crime_desc'] == 'HOMICIDE')]

import folium
nyc_totalcrime_map = folium.Map(location=[40.693943, -73.985880],
                       zoom_start=12,
                       tiles="CartoDB dark_matter")

for i in range(len(df_deadly)):
    lat = df_deadly.iloc[i]['Latitude']
    lon = df_deadly.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Crime', radius=30, color='#800040', fill=True).add_to(nyc_totalcrime_map)

nyc_totalcrime_map

"""

*  Defining new dataframes according boroughs.

"""

df_M = crime[crime['Borough'] == 'Manhattan']
df_R = crime[crime['Borough'] == 'Brooklyn']
df_Q = crime[crime['Borough'] == 'Queens']
df_B = crime[crime['Borough'] == 'Bronx']
df_S = crime[crime['Borough'] == 'Staten Island']

"""

* Defining new dataframes according to both borough and crime type


---



1. We use these dataframes in order to obtain distribution of crime types on different borughs.
2. With these dataframes we are able to determine which crime type is more common in which borugh. Then we can test out hypothesis borough by borough wise.



"""

df_S_Assault = df_S[df_S['Crime_desc'] == 'ASSAULT 3 & RELATED OFFENSES']
df_M_Assault = df_M[df_M['Crime_desc'] == 'ASSAULT 3 & RELATED OFFENSES']
df_R_Assault = df_R[df_R['Crime_desc'] == 'ASSAULT 3 & RELATED OFFENSES']
df_Q_Assault = df_Q[df_Q['Crime_desc'] == 'ASSAULT 3 & RELATED OFFENSES']
df_B_Assault = df_B[df_B['Crime_desc'] == 'ASSAULT 3 & RELATED OFFENSES']

df_S_Theft = df_S[df_S['Crime_desc'] == 'THEFT-FRAUD']
df_M_Theft = df_M[df_M['Crime_desc'] == 'THEFT-FRAUD']
df_R_Theft = df_R[df_R['Crime_desc'] == 'THEFT-FRAUD']
df_Q_Theft = df_Q[df_Q['Crime_desc'] == 'THEFT-FRAUD']
df_B_Theft = df_B[df_B['Crime_desc'] == 'THEFT-FRAUD']

df_S_Homicide = df_S[df_S['Crime_desc'] == 'HOMICIDE-NEGLIGENT-VEHICLE']
df_M_Homicide = df_M[df_M['Crime_desc'] == 'HOMICIDE-NEGLIGENT-VEHICLE']
df_R_Homicide = df_R[df_R['Crime_desc'] == 'HOMICIDE-NEGLIGENT-VEHICLE']
df_Q_Homicide = df_Q[df_Q['Crime_desc'] == 'HOMICIDE-NEGLIGENT-VEHICLE']
df_B_Homicide = df_B[df_B['Crime_desc'] == 'HOMICIDE-NEGLIGENT-VEHICLE']

df_S_Criminal = df_S[df_S['Crime_desc'] == 'CRIMINAL MISCHIEF & RELATED OF']
df_M_Criminal = df_M[df_M['Crime_desc'] == 'CRIMINAL MISCHIEF & RELATED OF']
df_R_Criminal = df_R[df_R['Crime_desc'] == 'CRIMINAL MISCHIEF & RELATED OF']
df_Q_Criminal = df_Q[df_Q['Crime_desc'] == 'CRIMINAL MISCHIEF & RELATED OF']
df_B_Criminal = df_B[df_B['Crime_desc'] == 'CRIMINAL MISCHIEF & RELATED OF']

df_S_SexCrimes = df_S[df_S['Crime_desc'] == 'SEX CRIMES']
df_M_SexCrimes = df_M[df_M['Crime_desc'] == 'SEX CRIMES']
df_R_SexCrimes = df_R[df_R['Crime_desc'] == 'SEX CRIMES']
df_Q_SexCrimes = df_Q[df_Q['Crime_desc'] == 'SEX CRIMES']
df_B_SexCrimes = df_B[df_B['Crime_desc'] == 'SEX CRIMES']

"""1. Crime type : ASSAULT 3 & RELATED OFFENSES
2. Crime type : THEFT-FRAUD
3. Crime type : CRIMINAL MISCHIEF & RELATED OF
4. Crime type : SEX CRIMES

#### NYC assult & related offenses map
"""

nyc_assault_map = folium.Map(location=[40.693943, -73.985880],
                       zoom_start=10,
                       tiles="CartoDB dark_matter")

for i in range(len(df_S_Assault)):
    lat = df_S_Assault.iloc[i]['Latitude']
    lon = df_S_Assault.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#800080', fill=True).add_to(nyc_assault_map)

for i in range(len(df_M_Assault)):
    lat = df_M_Assault.iloc[i]['Latitude']
    lon = df_M_Assault.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#900060', fill=True).add_to(nyc_assault_map)

for i in range(len(df_R_Assault)):
    lat = df_R_Assault.iloc[i]['Latitude']
    lon = df_R_Assault.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#600050', fill=True).add_to(nyc_assault_map)

for i in range(len(df_Q_Assault)):
    lat = df_Q_Assault.iloc[i]['Latitude']
    lon = df_Q_Assault.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#500030', fill=True).add_to(nyc_assault_map)

for i in range(len(df_B_Assault)):
    lat = df_B_Assault.iloc[i]['Latitude']
    lon = df_B_Assault.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#800080', fill=True).add_to(nyc_assault_map)

nyc_assault_map

"""#### NYC theft & fraud map"""

import folium
nyc_theft_map = folium.Map(location=[40.693943, -73.985880],
                       zoom_start=10,
                       tiles="CartoDB dark_matter")

for i in range(len(df_S_Criminal)):
    lat = df_S_Criminal.iloc[i]['Latitude']
    lon = df_S_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Theft', radius=20, color='#800080', fill=True).add_to(nyc_theft_map)

for i in range(len(df_M_Criminal)):
    lat = df_M_Criminal.iloc[i]['Latitude']
    lon = df_M_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Theft', radius=20, color='#900060', fill=True).add_to(nyc_theft_map)

for i in range(len(df_R_Criminal)):
    lat = df_R_Criminal.iloc[i]['Latitude']
    lon = df_R_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Theft', radius=20, color='#600050', fill=True).add_to(nyc_theft_map)

for i in range(len(df_Q_Criminal)):
    lat = df_Q_Criminal.iloc[i]['Latitude']
    lon = df_Q_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Theft', radius=20, color='#500030', fill=True).add_to(nyc_theft_map)

for i in range(len(df_B_Criminal)):
    lat = df_B_Criminal.iloc[i]['Latitude']
    lon = df_B_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Theft', radius=20, color='#800080', fill=True).add_to(nyc_theft_map)

nyc_theft_map

"""####NYC criminal mischief & related offenses MAP"""

nyc_criminal_map = folium.Map(location=[40.693943, -73.985880],
                       zoom_start=10,
                       tiles="CartoDB dark_matter")

for i in range(len(df_S_Criminal)):
    lat = df_S_Criminal.iloc[i]['Latitude']
    lon = df_S_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC criminal', radius=20, color='#800080', fill=True).add_to(nyc_criminal_map)

for i in range(len(df_M_Criminal)):
    lat = df_M_Criminal.iloc[i]['Latitude']
    lon = df_M_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC criminal', radius=20, color='#900060', fill=True).add_to(nyc_criminal_map)

for i in range(len(df_R_Criminal)):
    lat = df_R_Criminal.iloc[i]['Latitude']
    lon = df_R_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC criminal', radius=20, color='#600050', fill=True).add_to(nyc_criminal_map)

for i in range(len(df_Q_Criminal)):
    lat = df_Q_Criminal.iloc[i]['Latitude']
    lon = df_Q_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC criminal', radius=20, color='#500030', fill=True).add_to(nyc_criminal_map)

for i in range(len(df_B_Criminal)):
    lat = df_B_Criminal.iloc[i]['Latitude']
    lon = df_B_Criminal.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC criminal', radius=20, color='#800080', fill=True).add_to(nyc_criminal_map)

nyc_criminal_map

"""####NYC sex crime map"""

nyc_sexcrime_map = folium.Map(location=[40.693943, -73.985880],
                       zoom_start=10,
                       tiles="CartoDB dark_matter")

for i in range(len(df_S_SexCrimes)):
    lat = df_S_SexCrimes.iloc[i]['Latitude']
    lon = df_S_SexCrimes.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#800080', fill=True).add_to(nyc_sexcrime_map)

for i in range(len(df_M_SexCrimes)):
    lat = df_M_SexCrimes.iloc[i]['Latitude']
    lon = df_M_SexCrimes.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#900060', fill=True).add_to(nyc_sexcrime_map)

for i in range(len(df_R_SexCrimes)):
    lat = df_R_SexCrimes.iloc[i]['Latitude']
    lon = df_R_SexCrimes.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#600050', fill=True).add_to(nyc_sexcrime_map)

for i in range(len(df_Q_SexCrimes)):
    lat = df_Q_SexCrimes.iloc[i]['Latitude']
    lon = df_Q_SexCrimes.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#500030', fill=True).add_to(nyc_sexcrime_map)

for i in range(len(df_B_SexCrimes)):
    lat = df_B_SexCrimes.iloc[i]['Latitude']
    lon = df_B_SexCrimes.iloc[i]['Longitude']
    folium.CircleMarker(location=[lat, lon], popup='NYC Sex Crime', radius=20, color='#800080', fill=True).add_to(nyc_sexcrime_map)

nyc_sexcrime_map

"""#### HEAT MAP OF OVERALL & SPECIFIC CRIMES 

* **Heat map of crimes**


---



1. Again we divide crime types to sub crimes
2. We are able to see density of crime types amoung NYC's boroughs.









"""

df_SexCrimes = crime[crime['Crime_desc'] == 'SEX CRIMES']
df_Assault = crime[crime['Crime_desc'] == 'ASSAULT 3 & RELATED OFFENSES']
df_Theft = crime[crime['Crime_desc'] == 'THEFT-FRAUD']
df_Hemidice = crime[crime['Crime_desc'] == 'HOMICIDE-NEGLIGENT-VEHICLE']
df_Criminal = crime[crime['Crime_desc'] == 'CRIMINAL MISCHIEF & RELATED OF']

print("Heat map over cooridantes of crimes")
crime.plot.hexbin(x='Longitude', y='Latitude', gridsize=25)
plt.title("Overall crimes")
plt.show()

df_SexCrimes.plot.hexbin(x='Longitude', y='Latitude', gridsize=25)
plt.title("Sex Crimes")

df_Assault.plot.hexbin(x='Longitude', y='Latitude', gridsize=25)
plt.title("Assault")

df_Theft.plot.hexbin(x='Longitude', y='Latitude', gridsize=25)
plt.title("Theft-fraud")

df_Hemidice.plot.hexbin(x='Longitude', y='Latitude', gridsize=25)
plt.title("Homidice")

df_Criminal.plot.hexbin(x='Longitude', y='Latitude', gridsize=25)
plt.title("Criminal Mischied & Releated of")

"""#### MAPS BASED ON DISTINCT SPECIFICATIONS

* According to distribution of crimes based on their harshness, the most harshness crimes in Brooklyn. On the other hand, non-harshness crime types in Queens at the west side and Staten Island.
"""

new_map = crimeX.loc[(crimeX['X_COORD_CD']!=0)]
sns.lmplot('X_COORD_CD', 
           'Y_COORD_CD',
           data=new_map[:],
           fit_reg=False, 
           hue="LAW_CAT_CD",
           palette='Dark2',
           height=12,
           ci=2,
           scatter_kws={"marker": "D", 
                        "s": 10})
ax = plt.gca()
ax.set_title("According to Harshness of Crime")

new_map = crimeX.loc[(crimeX['X_COORD_CD']!=0)]
sns.lmplot('X_COORD_CD', 
           'Y_COORD_CD',
           data=new_map[:],
           fit_reg=False, 
           hue="PERP_SEX",
           palette='Dark2',
           height=12,
           ci=2,
           scatter_kws={"marker": "D", 
                        "s": 10})
ax = plt.gca()
ax.set_title("Crimes by District")

"""* Plot is dominated by Asians and White Hispanic races. """

new_map = crimeX.loc[(crimeX['X_COORD_CD']!=0)]
sns.lmplot('X_COORD_CD', 
           'Y_COORD_CD',
           data=new_map[:],
           fit_reg=False, 
           hue="PERP_RACE",
           palette='Dark2',
           height=12,
           ci=2,
           scatter_kws={"marker": "D", 
                        "s": 10})
ax = plt.gca()
ax.set_title("Crimes Based on Race")

"""# **We are planning to do:**
We are planning to do clarify relationship between Airbnb housing/price distribution and NYC crimes distribution, borough by borough. With determining this, also we are able to determine are there any correlation with specific crimes and Airbnb housing/price distribution. In other words we can specify which kind of crimes can manipulate Airbnb distributions and which kind of crimes cannot manipulate.

# Machine Learning Models

## Machine Learing Model for Airbnb Dataset
"""

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

price = df['price']

X = df['longitude'].values.reshape(-1,1)
price_values = price.values.reshape(-1,1)

X_train, X_test, y_train, y_test = train_test_split(X,price_values,random_state=42, test_size=0.20)
regress = linear_model.LinearRegression()

regress.fit(X_train, y_train)

price_pred = regress.predict(X_test)
price_pred

print('Coefficients: \n', regress.coef_)
print('Mean squared error: %.2f'% mean_squared_error(y_test, price_pred))
print('Coefficient of determination: %.2f'% r2_score(y_test, price_pred))

plt.title("longitude vs. price")
plt.scatter(X_test, y_test,  color='blue')
plt.plot(X_test, price_pred, color='red', linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()

"""* Since mean squared error is very high and also R^2 is very low, so that there is no clear correlation."""

X2 = df['minimum_nights'].values.reshape(-1,1)

X2_train, X2_test, y2_train, y2_test = train_test_split(X2,price_values,random_state=42, test_size=0.20)

regress2 = linear_model.LinearRegression()

regress2.fit(X2_train, y2_train)

y2_pred = regress2.predict(X2_test)
y2_pred

print('Coefficients: \n', regress2.coef_)
# The mean squared error
print('Mean squared error: %.2f'% mean_squared_error(y2_test, y2_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'% r2_score(y2_test, y2_pred))

# Plot outputs
plt.title("minimum_nights vs. price")
plt.scatter(X2_test, y2_test,  color='blue')
plt.plot(X2_test, y2_pred, color='red', linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()

"""* Since mean squared error is very high and also R^2 is very low, so that there is no clear correlation."""

X3 = df['availability_365'].values.reshape(-1,1)

X3_train, X3_test, y3_train, y3_test = train_test_split(X3,price_values,random_state=42, test_size=0.20)

regress3 = linear_model.LinearRegression()

regress3.fit(X3_train, y3_train)

y3_pred = regress3.predict(X3_test)
y3_pred

print('Coefficients: \n', regress2.coef_)
# The mean squared error
print('Mean squared error: %.2f'% mean_squared_error(y3_test, y3_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'% r2_score(y3_test, y3_pred))

# Plot outputs
plt.title("Availability_365 vs. Price")
plt.scatter(X3_test, y3_test,  color='blue')
plt.plot(X3_test, y3_pred, color='red', linewidth=3)

plt.xticks(())
plt.yticks(())

plt.show()

"""* Since mean squared error is very high and also R^2 is very low, so that there is no clear correlation.

## Machine Learning Model for Crime Dataset
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.naive_bayes import GaussianNB
#from sklearn.neighbors import KNeighborsClassifier 
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
#from sklearn import metrics
#from sklearn.metrics import classification_report
from sklearn import preprocessing

from sklearn.cluster import KMeans
import seaborn as sns

"""* Find the crime rates with group by districts and create new data based on the types of crimes.




"""

district_crime_rates = pd.DataFrame(columns=['theft_count', 'assault_count', 'sexual_offense_count', 
                                             'weapons_offense_count', 'criminal_offense_count', 
                                             'human_trafficking_count', 'narcotic_offense_count'])
district_crime_rates = district_crime_rates.astype(int) 

for i in range(1, 124):   # max number of district is 123
    temp_district_df = crime[crime['ARREST_PRECINCT'] == i] 

    temp_district_theft = temp_district_df[(temp_district_df['Crime_desc'] == 'THEFT-FRAUD') ] 
    num_theft = temp_district_theft.Crime_desc.count() 
    
    temp_district_assault = temp_district_df[temp_district_df['Crime_desc'] == 'ASSAULT 3 & RELATED OFFENSES'] 
    num_assault = temp_district_assault.Crime_desc.count()    
    
    temp_district_sexual_offense = temp_district_df[temp_district_df['Crime_desc'] == 'SEX CRIMES'] 
    num_sexual_offense = temp_district_sexual_offense.Crime_desc.count()
    
    temp_district_weapons_offense = temp_district_df[temp_district_df['Crime_desc'] == 'DANGEROUS WEAPONS'] 
    num_weapons_offense = temp_district_weapons_offense.Crime_desc.count()
    
    temp_district_criminal_offense = temp_district_df[(temp_district_df['Crime_desc'] == 'CRIMINAL MISCHIEF & RELATED OF') | (temp_district_df['Crime_desc'] == 'HOMICIDE-NEGLIGENT-VEHICLE')] 
    num_criminal_offense = temp_district_criminal_offense.Crime_desc.count()
    
    temp_district_human_trafficking = temp_district_df[(temp_district_df['Crime_desc'] == 'VEHICLE AND TRAFFIC LAW') | (temp_district_df['Crime_desc'] == 'INTOXICATED & IMPAIRED DRIVING')] 
    num_human_trafficking = temp_district_human_trafficking.Crime_desc.count()
    
    temp_district_narcotic_offense = temp_district_df[temp_district_df['Crime_desc'] == 'DANGEROUS DRUGS'] 
    num_narcotic_offense = temp_district_narcotic_offense.Crime_desc.count()
    
    #temp_district_other_offense = temp_district_df[temp_district_df['Crime_desc'] == 'OTHER_OFFENSE'] 
    #num_other_offense = temp_district_other_offense.Crime_desc.count()

    district_crime_rates.loc[i] = [num_theft, num_assault, num_sexual_offense, num_weapons_offense, num_criminal_offense, num_human_trafficking, num_narcotic_offense]    
    
district_crime_rates.head()

district_crime_rates_standardized = preprocessing.scale(district_crime_rates)
district_crime_rates_standardized = pd.DataFrame(district_crime_rates_standardized)

"""###KMeans Cluster

### Find the best value of k
"""

n_clusters_range = np.arange(1, 40)
# store intra cluster variation value
intra_cluster_var = []

for k in n_clusters_range:
    model = KMeans(n_clusters=k)
    model.fit(district_crime_rates_standardized)
    
    # append intra cluster variation, i.e. inertia attribute in the model
    intra_cluster_var.append(model.inertia_)
    
plt.plot(n_clusters_range, intra_cluster_var)
plt.xlabel("Number of CLusters")
plt.ylabel("Inertia")
plt.title("Elbow Method")
plt.show()

"""* Use k in order to divide the into 40 clusters
* Because line getting smoother after the 40





"""

kmeans = KMeans(n_clusters = 40, init = 'k-means++', random_state = 42) # we set cluster as 40 because we found line getting smoother after 40
y_kmeans = kmeans.fit_predict(district_crime_rates_standardized)
#y_kmeans

#beginning of the cluster numbering with 1 instead of 0
#y_kmeans1=y_kmeans+1

# New list called cluster
kmeans_clusters = list(y_kmeans)
# Adding cluster to our data set
district_crime_rates['kmeans_cluster'] = kmeans_clusters

#Mean of clusters 1 to 40
kmeans_mean_cluster = pd.DataFrame(round(district_crime_rates.groupby('kmeans_cluster').mean(),1))

district_crime_rates['district'] = district_crime_rates.index # we define district as index 
# we put k_mean_cluster data in our district_crime_rate data
district_crime_rates = district_crime_rates[['district', 'kmeans_cluster', 'theft_count', 'assault_count', 'sexual_offense_count', 'weapons_offense_count', 'criminal_offense_count', 'human_trafficking_count', 'narcotic_offense_count']]

district_crime_rates = district_crime_rates.drop(['theft_count', 'assault_count', 'sexual_offense_count', 'weapons_offense_count', 'criminal_offense_count', 'human_trafficking_count', 'narcotic_offense_count'], axis=1)
district_crime_rates.head(123)

crime = crime.rename(columns={'ARREST_PRECINCT':'district'})

"""* Let's merge the new dataset which is consist of the rate of crime by district and our main dataset with common district column. 

* Then, plot a map with kmeans clusters which is split the data into clusters by crime types and district.
"""

crimes_data_clustered = pd.merge(crime, district_crime_rates, on='district', how='inner')

"""### Map"""

new_crimes_data = crimes_data_clustered.loc[(crimes_data_clustered['X_COORD_CD']!=0)]
sns.lmplot('X_COORD_CD', 
           'Y_COORD_CD',
           data=new_crimes_data[:],
           fit_reg=False, 
           hue="kmeans_cluster",
           palette= sns.color_palette("hls", 40),
           height=12,
           ci=2,
           scatter_kws={"marker": "D", 
                        "s": 10})
ax = plt.gca()
ax.set_title("KMeans Clustering of Crimes by District")

"""* number of different colors in each borough gives us how many disticnt crimes that boroughs have. Therefore, higher color range means higher crimes rate.

* According to our observations, Manhattan, Bronx and Brooklyn are more dangerous places compared to other NYC boroughs.

### Models
"""

#Converting the numercial attributes to categorical attributes
crimeX.AGE_GROUP = pd.Categorical(crimeX.AGE_GROUP)
crimeX.PERP_SEX = pd.Categorical(crimeX.PERP_SEX)
crimeX.ARREST_DATE = pd.Categorical(crimeX.ARREST_DATE)
crimeX.PERP_RACE = pd.Categorical(crimeX.PERP_RACE)
crimeX.OFNS_DESC = pd.Categorical(crimeX.OFNS_DESC)
crimeX.ARREST_BORO = pd.Categorical(crimeX.ARREST_BORO)

crimes_data_prediction = crimeX.drop(['ARREST_DATE','ARREST_KEY','PD_CD','PD_DESC','KY_CD','LAW_CODE','LAW_CAT_CD','ARREST_PRECINCT','JURISDICTION_CODE','X_COORD_CD','Y_COORD_CD'],axis=1)

crimes_data_prediction.head()

crimes_data_prediction.info()

crimes_data_prediction = pd.get_dummies(crimes_data_prediction,drop_first=True)

crimes_data_prediction

#Train test split with a test set size of 20% of entire data
X_train, X_test, y_train, y_test = train_test_split(crimes_data_prediction.drop(['AGE_GROUP_<18'],axis=1),crimes_data_prediction['AGE_GROUP_<18'], test_size=0.2, random_state=42)

#Standardizing the data
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train) 
X_test = scaler.transform(X_test)

from sklearn.metrics import accuracy_score

"""####Gaussain Naive Bayes"""

classifier = GaussianNB()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

accuracy_score(y_test, y_pred)

conf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(conf_matrix)

sns.heatmap(conf_matrix, annot = True, fmt = ".3f", square = True, cmap = plt.cm.Blues)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion matrix')
plt.tight_layout()

"""####Decision Tree """

#Decision tree with Entropy as attribute measure
model = tree.DecisionTreeClassifier(criterion = "entropy", random_state = 42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy_score(y_test, y_pred)

# Compute confusion matrix
conf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(conf_matrix)

# Plot confusion matrix
sns.heatmap(conf_matrix, annot = True, fmt = ".3f", square = True, cmap = plt.cm.Blues)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion matrix')
plt.tight_layout()

"""####Random Forest"""

#Random Forest classifier  - Best one
model = RandomForestClassifier(n_estimators = 10,criterion='entropy',random_state=42)

model.fit(X_train,y_train)

y_pred = model.predict(X_test)

accuracy_score(y_test, y_pred)

# Compute confusion matrix
conf_matrix = metrics.confusion_matrix(y_test, y_pred)
print(conf_matrix)

# Plot confusion matrix
sns.heatmap(conf_matrix, annot = True, fmt = ".3f", square = True, cmap = plt.cm.Blues)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion matrix')
plt.tight_layout()

"""#RESULT & DISCUSSION


#Airbnb Dataset
*  Manhattan has the more expensive house's prices compared to other boroguhs of NYC.
*  Manhattan has wider range of house prices, altough it has wider prices, it has also higher prices.
*  Number of Entire home in Manhattan is more than others and this can be the reason of why Manhattan has more expensive house, because price of entire home is more expensive than other room types.
*  According to total number of houses in Manhattan, Manhattan is the most popular places for airbnb housing. And the foolowings are; Brooklyn, Queens, Bronx, Staten Island.


#Crime Dataset
* The most of the crimes in NYC found in Brooklyn And followings are; Manhattan, Bronx, Queen, Staten Island.
* Crime Rates in NYC in ascending order; Bronx, Manhattan, Brooklyn, Queens, Staten Island
* According to heat map of overall crimes, Bronx and Manhattan are leading(in order)
* According to distribution of crimes based on their harshness, the most hasrshness crimes in Brooklyn. On the other hand, non-harshness crime types in west side of the Queens and Staten Island.
* The most of the crimes in NYC are commited by Asian and White Hispanic. (So, Blacks are innocent :) )

#Machine Learning Models
* We cannot find clear correlation between our features and price distribution with using linear regression method 
* According to our observations based on KMeans, Manhattan, Bronx and Brooklyn are more dangerous places compared to other NYC boroughs.
* The best model that fit our cases is Random Forest because accuracy rate of Random Forest is bigger than others. We tried most of the cases, in some of them these three models perform very similar. but some cases Random Forest makes great differences. We try another models like kNN but we cannot fit our dataset.

#CONCLUSION
Our aim was "to find the relationship between price/housing distribution of Airbnb rentals and crimes in NYC." We observed that the most expensive airbnb houses are in Manhattan. According to our findings that related with crime rates, Manhattan stands in the second place with %3.3, altough it stands in second place, there is %0.1 difference between the first one. Also, since Manhattan has the most popular place for airbnb houses, we can say that people have tendency to rent houses in Manhattan, however, according to KMeans, Manhattan has the widest range of crime type and as we said before it has one of the most intense crime rate. Moreover, Staten Island has the least crime rates and least variety of crimes, but price of the airbnb houses is less than the other boroughs. Therefore, we cannot find any relation between price and crime distribution.
"""